不是只有好的算法就能成功
---
深度学习是机器学习领域的分支学科，而机器学习(Machine Learning,ML)这门AI中的交叉学科，历史与AI同样悠久。深度学习即深度神经网络(Deep Neural Networks DNN)是其中重要方法之一。

AI、机器学习、深度学习之间的关系可以这样简单概况：AI是目的，机器学习是方向，深度学习是路径。

机器学习、神经网络都不是新概念，深度学习严格说也不是新概念，但它在2006年被欣顿(Geoffery Hinton)等重新包装退出时，的确有很多创造性的新思想和算法，让机器学习焕然一新，但是AI历史告诉我们，不止仅有好的算法就能成功。

比如，深度学习用统计学方法巧妙处理深度神经网络数据的权重，把概率上相对近似度高的数据合并处理，大大降低了数据的维数，也降低了复杂度。由于这项关键技术，欣顿把自己构建的深度神经网络也称之为深度置信网络，但是大大降低了数据的维数，涉及的数据和计算量依然惊人。

神经网络模仿人大脑，用学习的方法获取准确知识并用之解决问题，最初的神经网络只有单层，这样简单的神经网络当然不实用，由于人脑的学习过程实际上是分层进行的，人工神经网络后来发展成多层神经网络，在输入层和输出层之间加上隐层，这就是三层神经网络，隐层还可以不断增加，达几层，几十层之多，夺得ImageNet2015年冠军的微软ResNet系统，加上152层的深度神经网络。

深度神经网络可以使学习一层层深入，从常识来看，有深度固然好，但随着神经网络层数的增加，计算量也变得无比巨大，又如，深度学习中涉及的数据需求，也会随着深度神经网络系统的规模增加而大大增强

就像人类学习需要各种学习材料，学习场景一样，机器学习也需要这些，只不过机器需要的学习材料、场景必须数字化

也像人类学习分为有老师学习和无老师的学习，机器学习分为监督学习和无监督学习，监督学习必须使用经过标注的数据，无监督学习不需要经过标注的数据

神经网络本质上是一个数据驱动模型，需要提供数据让机器去学习，然后根据学习的结果不断调整，优化模型中的参数，达到模型收敛，也就是达到预期的效果，这一过程叫做训练。

一个神经网络一般需要三个数据；即训练集、开发集、测试集，训练规模越大、匹配度越高，训练效果也就是学习效果越好。

如果神经网络系统的训练数据太少，会出现“过拟合”。这个专业术语的意思是结果太针对特定场景，不能推广，而学习结果能广泛适用，则成为“泛化”，正像人类举一反三是因为掌握了大量知识一样，机器学习也需要大量、多样化的数据。

形象的说，机器学习需要大量多样化的数据“喂”进机器，“喂”进机器数据越多，质量越好，学习效果越好。

欣顿想了很多办法来减少对监督学习的需求，因为这样意味着大大减少工作量和提高训练速度，它的创新思路是将无监督学习和有监督学习结合使用。首先让机器进行无监督学习，即自动处理没有标注过的数据，进行逐层预训练，使得在利用反向传播算法对网络进行全局优化之前，网络参数能达到一个好的起始点，从而在训练完成时达到一个较好的收敛点。最后阶段进行监督学习，利用经过标注的数据进行训练，这样大大提高了训练速度，欣顿形象的比喻说：“想象一下小孩子，当他们学着辨识牛时，并非需要看几百万张妈妈们标注上牛的图片，他们仅仅是自己学习观察牛的样子，然后问道“这是什么？”，妈妈会说这是牛，他们就学会了。”

深度学习巧妙的学习过程，即使有这些发明，深度学习也必须在计算能力大大增强和海量数据出现的情况下能充分发挥作用。

此外如果使神经网络系统表现的更加优秀，会涉及到一个有趣的术语，即鲁棒性(Robust)，这个英文术语音译非常传神，让人一看就知道含义粗壮、稳定。怎样提高神经网络系统的鲁棒性呢？常用方法之一是通过人为添加一定的噪音进行训练，就好比军人训练要增加一些恶劣的场景来提高战斗力一样，在神经网络中，为提高鲁棒性来添加噪声，会增加数据量和多样性，同样提高了对计算能力的要求。

深度学习像一只雄鹰，需要两只翅膀，这两只翅膀就是GPU强大的算力和ImageNet大数据集

GPU从开始就考虑支持单指令多数据流，所以GPU在大规模并行计算方面远高于CPU，在执行特定任务时，一般认为GUP速度是CPU的100到300倍，深度学习涉及的计算刚好特定，主要进行高速度、大规模的矩阵运算。