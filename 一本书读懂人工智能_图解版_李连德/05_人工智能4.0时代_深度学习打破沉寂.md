5.1 深度学习开创新时代
---

21世纪，人工智能领域发生了两件大事，一个是2006年，一个是2012年。

2006年杰弗里.希尔顿等人提出了“深度学习(Deep Learning)”概念，什么是深度学习，其实它是机器学习的一个分支，属于无监督学习的一种，通过教授计算机深度学习，就能解决深层机构优化的问题，与此同时，燕乐存等人还提出了卷积神经网络，这种多层结构学习算法与深度学习结合就能利用空间的相对关系减少参数的数量，以此来提高计算机的训练性能，放到现在，就好比圆形这种平面图形与圆柱体这种立体图形的交集，但圆柱体也有属于自己的结构，深度学习就是通过这种区分来缩减计算机需要参考的参数数量。

深度学习建立在模拟人脑进行分析学习的神经网络基础上，虽然它是无监督学习的一种，但是它自身也具有监督学习和无监督学习之分，这不矛盾，就像人类属于高级智慧动物一样，在不同的学习框架下，深度学习需要建立的学习系统是不同的，卷积神经网络只是其中一种。

深度学习难点在于深度，什么是深度？机器学习过程是数据从一个输入端经过节点到输出端，

输入端   隐含量  输出端
 
输入端   隐含量  输出端

输入端   隐含量  输出端

其中每一个节点都会有一个相应的算法和得出的数值，当数据最终从输出端输出的时候，一整条路径就组成了一个函数，可以看做一个分类区间，而深度学习比机器学习更看重纵向传输，也就是在某一个节点进行深度计算，找到一个输入点到输出点的最长路径，这正是它与其它算法找到捷径有所不同的地方。

2012年在国际性图像识别大赛ILSVRC(Large Scale Visual Recognition Challenge)上，由多伦多大学开发的Super Vision系统超越了机器学习时代“老掉牙”的研究领域，在实际应用上，通过机器学习，也就是人工输入特征量这种方法，即便是最高级的技术系统，错误率也在26%以上，但是Super Vision系统错误率仅为15%左右，该系统之所以能将错误率降低10%就是因为采用了深度学习，使计算机能够自动识别特征量，要知道很多研究者聚在一起，一年的时间也许只能将这种错误率降低1%

5.2 自动编码器：输入与输出相同
---

目前技术来看，人工神经网络无疑是机器学习十分有效的算法之一，这种算法不但具有十分强的非线性拟合能力，而且具有极高的容错能力，能从自然环境中的不确定因素里提取数据，并整理出符合条件的规则。但是在实际中，如果想要不断提高人工神经网络的这种泛化性，就要不断增加神经网络的层级数量，也就是我们前面提到过的隐层，隐层数量越多，人工神经网络处理的数据也就越多，这和人大脑一样，大脑开发越发达，脑沟就越多，而我们记忆处理的信息越多。

隐层越多，人工神经网络的自由度越大，所能表示的函数也就越多，那么多层级的人工神经网络肯定比一层两层的神经网络处理的效果好，但实际真的如此吗？在实际的测试过程中，多层神经网络的实效并不高，这是因为层数增多，无法保证内容的精准度。

如果神经网络层级增多，那么误差反向传递就无法抵达下面的层级，或者可以说，无法准确将错误内容传递给下级进行修正，公司颁布了新消息以后，会一个部门一个部门逐层传递下去，但是消息以口头形式传达，难免会导致信息失真，因此公司在颁布重要通知的时候，多是以文字或者会以的形式传达给员工，神经网络无法在进行数据分类的时候将这些信息实体化，所以层级越多，产生误差概率越高。

随着技术的不断发展和优化，人工神经网络确实实现了层层结构，这种结构是如何确保误差反向传播时的精准性呢？

加拿大人工智能杰出研究学者，深度学习概念提出人杰弗里.希尔顿进一步优化了神经网络层级技术，继而产生了一种解决方案--自动编码器

其实早在1986年就产生了自动编码器，但当时这种技术用在高维度复杂的数据处理上，随着深度学习概念的提出以及层级问题的出现，希尔顿等人在2006年对自动编码技术进行了改进，他们先用无监督逐层训练完成对隐层的测试，然后用误差反向传播对整个神经网络进行优化，调整，这种改良的新型算法能够有效解决局部失真问题。

