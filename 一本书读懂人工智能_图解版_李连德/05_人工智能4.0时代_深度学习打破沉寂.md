5.1 深度学习开创新时代
---

21世纪，人工智能领域发生了两件大事，一个是2006年，一个是2012年。

2006年杰弗里.希尔顿等人提出了“深度学习(Deep Learning)”概念，什么是深度学习，其实它是机器学习的一个分支，属于无监督学习的一种，通过教授计算机深度学习，就能解决深层机构优化的问题，与此同时，燕乐存等人还提出了卷积神经网络，这种多层结构学习算法与深度学习结合就能利用空间的相对关系减少参数的数量，以此来提高计算机的训练性能，放到现在，就好比圆形这种平面图形与圆柱体这种立体图形的交集，但圆柱体也有属于自己的结构，深度学习就是通过这种区分来缩减计算机需要参考的参数数量。

深度学习建立在模拟人脑进行分析学习的神经网络基础上，虽然它是无监督学习的一种，但是它自身也具有监督学习和无监督学习之分，这不矛盾，就像人类属于高级智慧动物一样，在不同的学习框架下，深度学习需要建立的学习系统是不同的，卷积神经网络只是其中一种。

深度学习难点在于深度，什么是深度？机器学习过程是数据从一个输入端经过节点到输出端，

输入端   隐含量  输出端
 
输入端   隐含量  输出端

输入端   隐含量  输出端

其中每一个节点都会有一个相应的算法和得出的数值，当数据最终从输出端输出的时候，一整条路径就组成了一个函数，可以看做一个分类区间，而深度学习比机器学习更看重纵向传输，也就是在某一个节点进行深度计算，找到一个输入点到输出点的最长路径，这正是它与其它算法找到捷径有所不同的地方。

2012年在国际性图像识别大赛ILSVRC(Large Scale Visual Recognition Challenge)上，由多伦多大学开发的Super Vision系统超越了机器学习时代“老掉牙”的研究领域，在实际应用上，通过机器学习，也就是人工输入特征量这种方法，即便是最高级的技术系统，错误率也在26%以上，但是Super Vision系统错误率仅为15%左右，该系统之所以能将错误率降低10%就是因为采用了深度学习，使计算机能够自动识别特征量，要知道很多研究者聚在一起，一年的时间也许只能将这种错误率降低1%

5.2 自动编码器：输入与输出相同
---

目前技术来看，人工神经网络无疑是机器学习十分有效的算法之一，这种算法不但具有十分强的非线性拟合能力，而且具有极高的容错能力，能从自然环境中的不确定因素里提取数据，并整理出符合条件的规则。但是在实际中，如果想要不断提高人工神经网络的这种泛化性，就要不断增加神经网络的层级数量，也就是我们前面提到过的隐层，隐层数量越多，人工神经网络处理的数据也就越多，这和人大脑一样，大脑开发越发达，脑沟就越多，而我们记忆处理的信息越多。

隐层越多，人工神经网络的自由度越大，所能表示的函数也就越多，那么多层级的人工神经网络肯定比一层两层的神经网络处理的效果好，但实际真的如此吗？在实际的测试过程中，多层神经网络的实效并不高，这是因为层数增多，无法保证内容的精准度。

如果神经网络层级增多，那么误差反向传递就无法抵达下面的层级，或者可以说，无法准确将错误内容传递给下级进行修正，公司颁布了新消息以后，会一个部门一个部门逐层传递下去，但是消息以口头形式传达，难免会导致信息失真，因此公司在颁布重要通知的时候，多是以文字或者会以的形式传达给员工，神经网络无法在进行数据分类的时候将这些信息实体化，所以层级越多，产生误差概率越高。

随着技术的不断发展和优化，人工神经网络确实实现了层层结构，这种结构是如何确保误差反向传播时的精准性呢？

加拿大人工智能杰出研究学者，深度学习概念提出人杰弗里.希尔顿进一步优化了神经网络层级技术，继而产生了一种解决方案--自动编码器

其实早在1986年就产生了自动编码器，但当时这种技术用在高维度复杂的数据处理上，随着深度学习概念的提出以及层级问题的出现，希尔顿等人在2006年对自动编码技术进行了改进，他们先用无监督逐层训练完成对隐层的测试，然后用误差反向传播对整个神经网络进行优化，调整，这种改良的新型算法能够有效解决局部失真问题。

简单来说，自动编码器的功能是为计算机学习过程提供一个学习正解的机会，例如，我们在输入“冬”字的时候，自动编码程序会将整个手写字体“冬”同时传输到输入层和输出层。

自动编码器的这种功能看起来像多此一举，自动编码器真的在做无用功吗？其实只要认真分析，就能发现其中的绝妙的细节处理，就像霍金斯先生说的“用旧香蕉换取新香蕉”，自动编码器的功能就是在现有的数据中将这些“旧香蕉”进行分类综合，从而缩减神经网络之间所需要传递的信息量，以此来减少口口相传的中间过程，保证信息的精度。

人工神经网络虽然提高信息处理能力，但是在处理过程中会将同类信息分开处理，例如：有两个阈值都代表紫色，自动编码器通过归类将这两个信息都归纳为紫色。输出结果也只有紫色一种。

哪些数据经过归类综合处理以后不会出现偏差？哪些会产生很大的偏差，在加入自动编码器以后，这正是计算机需要学习的内容，这些内容计算机掌握以后，误差会越来越小，具有这些功能的人工智能可以进行多层级的操作，而最终结果也必然是人工智能采用的特征量越来越准。

5.3 多层架构深度挖掘
---

深度学习能够帮助计算机分辨出来自然界中混乱信息的特征量，而自动编码器能让这些特征量变得更加工整，这两者是如何协同工作的呢？人工智能4.0时代深度学习又怎样让人工智能更加智能呢？

在深度学习中，多层级架构是一个重点攻克的领域，在这个领域里，自然界的数据被计算机接收后，自动编码器就会在一定程度上取代原有的输入层，因此我们可以将隐层作为第一层，并将输出层作为第二层，隐层的作用就是将正确分类的数据输出给计算机，让其自我学习，如果我们假定神经网络接收到的信息有100个维度，那么经过自动编码器和隐层处理以后，100个维度变成10个维度，随后将10个维度将作为下一个隐层(可以被认为是下一层输入层)并再度处理。

在第二个隐层中，数据被压缩后得到了质量更高的特征量，在进入下一节点或者说另一层节点的时候，这个数据在自动编码器的调解下，再度还原为100个维度的信息，也就是说数据被反向还原了。在进入另一层级后，这些数据将再度被分成不同维度的信息，最终在多个节点的共同作用下，数据被分成众多输出端“已学习”内容。

数据一层层分类学习后，最终形成一个个独立信息，对计算机来说，学习这样独立信息所话费的时间比集合学习某一些自然信息要少的多，当分层数量为无数层以后，特征量也变得极度清晰，也就是说，第一层计算机学习的是“脚”；而到了第二层就变成“人类的脚”；第三层就是“五个脚趾的脚”，......一旦计算机能够识别“特征1、2、3”这些不变量，那么我们只要将这些概念的名称交给它就行了，在有监督学习过程中，这样的概念传授一件十分简单的事。

将一些有关联的事结合在一起，组成一个具体的特征量，再从这些特征量中提取更多高级的特征量，最后给这些高级的特征量标注一个概念，就是深度学习与自动编码器协同工作的最终目的，

5.4 情绪识别里面的“深度学习”
---

google的猫脸识别系统，就充分利用深度学习能够自主学习并分析出特征量，以此来辨别那张是猫脸，让人工智能告别脸盲，